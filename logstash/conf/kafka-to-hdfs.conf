input {
kafka {
 #读取生产环境kafka_2.10-0.10.1.0 topic中的数据
 bootstrap_servers => "bigdata-1:9095,bigdata-2:9095,bigdata-3:9095,bigdata-4:9095,bigdata-5:9095,bigdata-6:9095"
 group_id => "helios-test-group-2017022720xxx"
 topics => ["xxxxxx"]
 codec => "json"
 consumer_threads => 6
 }
}

output {
  webhdfs {
    host => "bigdata-13"
    port => 50070
    standby_host => "bigdata-1"
    standby_port => 50070
    path => "/test2/xxxxxx/testxxx.log"
    user => "abc"
    codec => "json"
  }

  #used to compare file which send to Hdfs
  file {
    #codec can't be json and plain,just not set
    path => "/data/logs/logstash_v5/xxxxxx/helios-output.log"
  }
}
使用logstash的webhdfs插件，用来将kafka的日志写入Hdfs中，是不能用于生产环境的。
因为当强行kill -9 这个logstash进程，再次启动，发现日志重复写入了。
测试3万行日志，像上面操作试验，最终写入hdfs的日志条数是3万2千多行。

耗费将近1天的时间做这个事情，准备测试数据，插件配置等，测试结果是插件不满足线上使用。
吸取的经验教训:首先用脑去思考，logstash里的kafka input插件，在强行kill后，是否能准确记录offset位置。
以及webhdfs插件的可用性（所依赖的hdfs版本，以及本身所依赖的webhdfs的最新更新时间），本以为一两个小时就可以搞定，
结果陷入进去，浪费了时间，切记。
kafka input插件在kill -9 模式的时候，会有offset记录问题，当再次启动kafka input插件的时候，offset回退一定位置，导致了
消息重新消费。
